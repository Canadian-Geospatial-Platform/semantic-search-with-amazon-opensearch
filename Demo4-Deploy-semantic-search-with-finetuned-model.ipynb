{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcc816a-90fb-4740-9df4-0b2a1bdee6bf",
   "metadata": {},
   "source": [
    "### Deploy semantic search using with finetuned model \n",
    "The deployment architecture includes: \n",
    "- Choose a pretrain BERT model, here we use all-MiniLM-L6-v2 model\n",
    "- Save the ML models in S3 bucket\n",
    "- Host the ML models using SageMaker endpoints \n",
    "- Create Vector index and load data into the index \n",
    "- Create API gateway handels queries from web applications and pass it to lambda \n",
    "- Create a Lambda function to call SageMaker endpoints to generate embeddings from user query, and send the query results back to API gateway \n",
    "- API gateway sends the search results to frontend, and return search results to the users \n",
    "\n",
    "![Semantic_search_finetuned_fullstack](image/Semantic_search_finetune_fullstack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5c6b7-5e9a-450f-9227-9efe5e88f536",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.Preprocess and embed the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "289bb293-0c7f-40d9-a13e-4baea9456dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import boto3\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from inference import model_fn, predict_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2813f4a6-f69d-4250-ae7c-fdf1cabd33f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "def read_parquet_from_s3_as_df(region, s3_bucket, s3_key):\n",
    "    \"\"\"\n",
    "    Load a Parquet file from an S3 bucket into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - region: AWS region where the S3 bucket is located.\n",
    "    - s3_bucket: Name of the S3 bucket.\n",
    "    - s3_key: Key (path) to the Parquet file within the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame containing the data from the Parquet file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup AWS session and clients\n",
    "    session = boto3.Session(region_name=region)\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # Load the Parquet file as a pandas DataFrame\n",
    "    object = s3.Object(s3_bucket, s3_key)\n",
    "    body = object.get()['Body'].read()\n",
    "    df = pd.read_parquet(io.BytesIO(body))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Upload the duplicate date to S3 as a parquet file \n",
    "def upload_df_to_s3_as_parquet(df, bucket_name, file_key):\n",
    "    # Save DataFrame as a Parquet file locally\n",
    "    parquet_file_path = 'temp.parquet'\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Upload the Parquet file to S3 bucket\n",
    "    try:\n",
    "        response = s3_client.upload_file(parquet_file_path, bucket_name, file_key)\n",
    "        os.remove(parquet_file_path)\n",
    "        print(f'Uploading {file_key} to {bucket_name} as parquet file')\n",
    "        # Delete the local Parquet file\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "# Create new column 'organization_en' required by the API JSON response \n",
    "def extract_organisation_en(contact_str):\n",
    "    try:\n",
    "        # Parse the stringified JSON into Python objects\n",
    "        contact_data = json.loads(contact_str)\n",
    "        # If the parsed data is a list, iterate through it\n",
    "        if isinstance(contact_data, list):\n",
    "            for item in contact_data:\n",
    "                # Check if 'organisation' and 'en' keys exist\n",
    "                if 'organisation' in item and 'en' in item['organisation']:\n",
    "                    return item['organisation']['en']\n",
    "        elif isinstance(contact_data, dict):\n",
    "            # If the data is a dictionary, extract 'organisation' in 'en' directly\n",
    "            return contact_data.get('organisation', {}).get('en', None)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle cases where the contact string is not valid JSON\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other unexpected errors\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Text preprocess\n",
    "def preprocess_records_into_text(df):\n",
    "    selected_columns = ['features_properties_title_en','features_properties_description_en','features_properties_keywords_en']\n",
    "    df = df[selected_columns]\n",
    "    return df.apply(lambda x: f\"{x['features_properties_title_en']}\\n{x['features_properties_description_en']}\\nkeywords:{x['features_properties_keywords_en']}\",axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bcff7e3e-fc15-46d4-ae58-b43fba44eb56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30729/4081956423.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en['organisation_en'] = df_en['features_properties_contact'].apply(extract_organisation_en)\n",
      "/tmp/ipykernel_30729/4081956423.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en[columns_to_replace] = df_en[columns_to_replace].replace(values_to_replace)\n",
      "/tmp/ipykernel_30729/4081956423.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en['temporalExtent'] = df_en.apply(lambda row: {'begin': row['features_properties_temporalExtent_begin'], 'end': row['features_properties_temporalExtent_end']}, axis=1)\n"
     ]
    }
   ],
   "source": [
    "#1) Step1: Load the data \n",
    "df_parquet = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-geocore-geojson-to-parquet-dev', 'records.parquet')\n",
    "df_sentinel1 = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-geocore-geojson-to-parquet-dev', 'sentinel1.parquet')\n",
    "df = pd.concat([df_parquet, df_sentinel1], ignore_index=True)\n",
    "\n",
    "#2) Step2: Clean the data  \n",
    "col_names_list = [\n",
    "    'features_properties_id','features_geometry_coordinates','features_properties_title_en',\n",
    "    'features_properties_description_en','features_properties_date_published_date',\n",
    "    'features_properties_keywords_en','features_properties_options','features_properties_contact',\n",
    "    'features_properties_topicCategory','features_properties_date_created_date',\n",
    "    'features_properties_spatialRepresentation','features_properties_type',\n",
    "    'features_properties_temporalExtent_begin','features_properties_temporalExtent_end',\n",
    "    'features_properties_graphicOverview','features_properties_language','features_popularity',\n",
    "    'features_properties_sourceSystemName','features_properties_eoCollection',\n",
    "    'features_properties_eoFilters'\n",
    "]\n",
    "df_en = df[col_names_list]\n",
    "df_en['organisation_en'] = df_en['features_properties_contact'].apply(extract_organisation_en)\n",
    "\n",
    "# Create a new column 'temporalExtent' as a dictionary of {'begin': ..., 'end': ...}\n",
    "values_to_replace = {'Present': None, 'Not Available; Indisponible': None}\n",
    "columns_to_replace = ['features_properties_temporalExtent_begin', 'features_properties_temporalExtent_end']\n",
    "df_en[columns_to_replace] = df_en[columns_to_replace].replace(values_to_replace)\n",
    "\n",
    "df_en['temporalExtent'] = df_en.apply(lambda row: {'begin': row['features_properties_temporalExtent_begin'], 'end': row['features_properties_temporalExtent_end']}, axis=1)\n",
    "df_en = df_en.drop(columns =['features_properties_temporalExtent_begin', 'features_properties_temporalExtent_end'])\n",
    "\n",
    "values_to_replace = {'Not Available; Indisponible': None} # modifies dates to acceptable values\n",
    "columns_to_replace = ['features_properties_date_published_date', 'features_properties_date_created_date']\n",
    "df_en[columns_to_replace] = df_en[columns_to_replace].replace(values_to_replace)\n",
    "\n",
    "#3) Step 3: Preprocess text \n",
    "df_en['text'] = preprocess_records_into_text(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee73ac05-8069-418a-a4b8-baa6d1458ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Canadian Digital Elevation Model, 1945-2011\n",
      "This collection is a legacy product that is no longer supported. It may not meet current government standards.\\n\\nThe Canadian Digital Elevation Model (CDEM) is part of Natural Resources Canada's altimetry system designed to better meet the users' needs for elevation data and products.\\n\\nThe CDEM stems from the existing Canadian Digital Elevation Data (CDED). In these data, elevations can be either ground or reflective surface elevations.\\n\\nA CDEM mosaic can be obtained for a pre-defined or user-defined extent. The coverage and resolution of a mosaic varies according to latitude and to the extent of the requested area.\\nDerived products such as slope, shaded relief and colour shaded relief maps can also be generated on demand by using the Geospatial-Data Extraction tool. Data can then be saved in many formats.\\n\\nThe pre-packaged GeoTiff datasets are based on the National Topographic System of Canada (NTS) at the 1:250 000 scale; the NTS index file is available in the Resources section in many formats.\n",
      "keywords:Canada, Earth Sciences, elevation, relief, geomatics, GeoBase, CDEM, DEM, CDED, Geographic information systems, Geomatics, Earth sciences, Cartography, Geography, Geographic data, Topography\n"
     ]
    }
   ],
   "source": [
    "df_en.describe()\n",
    "print(type(df_en['text'].head(6)[1]))\n",
    "print(df_en['text'].head(6)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3759feae-5f5a-4ae4-b084-b456f31dc758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ec2-user/SageMaker/semantic-search-with-aws-opensearch/src/model/all-mpnet-base-v2-mpf-huggingface were not used when initializing MPNetModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing MPNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 63661/63661 [2:12:54<00:00,  7.98it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Embedding text \n",
    "tqdm.pandas()\n",
    "model_directory =\"/home/ec2-user/SageMaker/semantic-search-with-aws-opensearch/src/model/all-mpnet-base-v2-mpf-huggingface\"\n",
    "model = model_fn(model_directory)\n",
    "df_en['vector'] = df_en['text'].progress_apply(lambda x: predict_fn({\"inputs\": x}, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2302fb0f-cc83-4f7d-afce-78dff9325afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "768\n",
      "(63661,)\n"
     ]
    }
   ],
   "source": [
    "vector = df_en['vector'].head(6) \n",
    "#print(vector[0])\n",
    "print(type(vector[0]))\n",
    "print(len(vector[0]))\n",
    "print(df_en['vector'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "367ebc98-e96f-4d4c-92d1-d7987f3d0d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading semantic_search_embeddings.parquet to webpresence-nlp-data-preprocessing-dev as parquet file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5 Upload the embeddings as a parquet file to S3 bucket \n",
    "upload_df_to_s3_as_parquet(df=df_en, bucket_name='webpresence-nlp-data-preprocessing-dev', file_key='semantic_search_embeddings.parquet') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22038965-7c6d-4ae8-95ac-5056e417a960",
   "metadata": {},
   "source": [
    "### 2. Create OpenSearch index and load text/vector data into the index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983494e2-60fc-4e56-bf2f-18d3d7cf5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import time\n",
    "# import boto3\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from urllib.parse import urlparse\n",
    "# from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from opensearch import get_awsauth_from_secret, create_opensearch_connection, delete_aos_index_if_exists, load_data_to_opensearch_index\n",
    "from Preprocess_and_embed_text import read_parquet_from_s3_as_df\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198be8c-43a6-4b0a-8dc8-03f5b6bf343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: read the embedding data from the S3 bucket \n",
    "df_en = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-nlp-data-preprocessing-dev', 'semantic_search_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee551457-b480-460e-8f87-4e9b94c05db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to OpenSearch established: <OpenSearch([{'host': 'search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com', 'port': 443}])>\n"
     ]
    }
   ],
   "source": [
    "# Create a opensearch connection \n",
    "# region = environ['MY_AWS_REGION']\n",
    "# aos_host = environ['OS_ENDPOINT'] \n",
    "# os_secret_id = environ['OS_SECRET_ID']\n",
    "\n",
    "region = \"ca-central-1\"\n",
    "aos_host = \"search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com\"\n",
    "os_secret_id = \"dev/OpenSearch/SemanticSearch\"\n",
    "\n",
    "awsauth = get_awsauth_from_secret(region, secret_id=os_secret_id)\n",
    "aos_client =create_opensearch_connection(aos_host, awsauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa9b4c18-76ce-45ba-abf7-fe833236b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index \n",
    "index_name = \"mpnet-mpf-knn\"\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True, #This enables the k-nearest neighbor (KNN) search capability on the index.\n",
    "        \"index.knn.space_type\": \"cosinesimil\", #cosine similarity \n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"coordinates\":{\n",
    "              \"type\": \"geo_shape\", \n",
    "              \"store\": True \n",
    "            }  \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "528ef036-b3a1-4142-ae29-86828feff5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current indexes: ['nlp_knn', '.opensearch-observability', '.plugins-ml-config', 'search', 'keyword_search', '.ql-datasources', '.opendistro_security', '.kibana_1', 'mpnet-mpf-knn']\n",
      "Deleted index: mpnet-mpf-knn\n",
      "Response: {'acknowledged': True}\n",
      "Indexes after deletion attempt: ['nlp_knn', '.opensearch-observability', '.plugins-ml-config', 'search', 'keyword_search', '.ql-datasources', '.opendistro_security', '.kibana_1']\n"
     ]
    }
   ],
   "source": [
    "#Delete index if it exists \n",
    "delete_aos_index_if_exists(aos_client, index_to_delete=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ccc73b1-0254-4f05-aa39-08d1022bb747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'mpnet-mpf-knn'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a index \n",
    "aos_client.indices.create(index=index_name,body=knn_index,ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9eae90f8-a870-4fb4-8a45-2c0576ebdeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:   9%|▉         | 5674/63661 [01:34<14:32, 66.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n",
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:   9%|▉         | 5702/63661 [01:35<14:34, 66.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n",
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:   9%|▉         | 5746/63661 [01:35<13:57, 69.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n",
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:   9%|▉         | 5768/63661 [01:36<14:21, 67.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  10%|▉         | 6271/63661 [01:43<15:33, 61.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  10%|▉         | 6358/63661 [01:45<13:04, 73.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 6780/63661 [01:51<14:22, 65.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 6831/63661 [01:52<14:08, 67.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 6981/63661 [01:54<14:07, 66.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 7011/63661 [01:55<14:04, 67.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 7035/63661 [01:55<14:46, 63.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 7067/63661 [01:56<13:51, 68.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  11%|█         | 7126/63661 [01:56<11:44, 80.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  12%|█▏        | 7901/63661 [02:09<12:50, 72.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  13%|█▎        | 8142/63661 [02:13<13:01, 71.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  13%|█▎        | 8348/63661 [02:16<14:52, 61.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n",
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  13%|█▎        | 8368/63661 [02:17<17:12, 53.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  13%|█▎        | 8381/63661 [02:17<16:03, 57.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  13%|█▎        | 8404/63661 [02:17<13:59, 65.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  13%|█▎        | 8438/63661 [02:18<15:19, 60.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  14%|█▎        | 8610/63661 [02:21<20:08, 45.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  14%|█▎        | 8687/63661 [02:22<14:37, 62.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  14%|█▍        | 9034/63661 [02:29<13:50, 65.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  14%|█▍        | 9170/63661 [02:31<14:35, 62.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  14%|█▍        | 9212/63661 [02:32<15:44, 57.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  15%|█▍        | 9254/63661 [02:33<15:23, 58.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  15%|█▍        | 9434/63661 [02:36<13:56, 64.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records:  53%|█████▎    | 33567/63661 [09:26<13:45, 36.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestError(400, 'mapper_parsing_exception', 'failed to parse field [coordinates] of type [geo_shape]')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Records: 100%|██████████| 63661/63661 [18:19<00:00, 57.91it/s]\n"
     ]
    },
    {
     "ename": "ConnectionTimeout",
     "evalue": "ConnectionTimeout caused by - ReadTimeout(HTTPSConnectionPool(host='search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com', port=443): Read timed out. (read timeout=10))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/urllib3/connectionpool.py:358\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/connection/http_requests.py:191\u001b[0m, in \u001b[0;36mRequestsHttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, allow_redirects, ignore, headers)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/requests/adapters.py:532\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load data to OpenSearch Index \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mload_data_to_opensearch_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_en\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maos_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/semantic-search-with-aws-opensearch/src/opensearch.py:131\u001b[0m, in \u001b[0;36mload_data_to_opensearch_index\u001b[0;34m(df_en, aos_client, index_name, log_level)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Final record count check (Optional, can slow down the script if the index is large)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43maos_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch_all\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted indexing. Records loaded into the index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Total time taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/client/utils.py:180\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m             params[p] \u001b[38;5;241m=\u001b[39m _escape(v)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/client/__init__.py:1748\u001b[0m, in \u001b[0;36mOpenSearch.search\u001b[0;34m(self, body, index, params, headers)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1746\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_make_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/transport.py:447\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# connection didn't fail, confirm its live status\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\u001b[38;5;241m.\u001b[39mmark_live(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/transport.py:408\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    405\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_connection()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     status, headers_response, data \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     headers_response \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    420\u001b[0m         header\u001b[38;5;241m.\u001b[39mlower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers_response\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    421\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/connection/http_requests.py:208\u001b[0m, in \u001b[0;36mRequestsHttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, allow_redirects, ignore, headers)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e), e)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, requests\u001b[38;5;241m.\u001b[39mTimeout):\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIMEOUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e), e)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e), e)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# raise warnings if any from the 'Warnings' header.\u001b[39;00m\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m: ConnectionTimeout caused by - ReadTimeout(HTTPSConnectionPool(host='search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com', port=443): Read timed out. (read timeout=10))"
     ]
    }
   ],
   "source": [
    "#Load data to OpenSearch Index \n",
    "load_data_to_opensearch_index(df_en, aos_client, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "353f5d44-f02e-4645-939d-a3b3951736fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records loaded into the index mpnet-mpf-knn is 10000.\n"
     ]
    }
   ],
   "source": [
    "#Check \n",
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(f\"Records loaded into the index {index_name} is {res['hits']['total']['value']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3585b-389c-4388-a069-e52d7ce980aa",
   "metadata": {},
   "source": [
    "### 3. Deploy all-mpnet-base-v2-mpf-huggingface model using sagemaker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457c3d9-d25d-475b-90e0-cb4c68cf43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a378803-1774-487b-9c98-4534e69ddc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from model upload: s3://sagemaker-ca-central-1-006288227511/sentence-transformers-model/all-mpnet-base-v2-mpf-huggingface.tar.gz\n",
      "----------------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model/all-mpnet-base-v2-mpf-huggingface.tar.gz', key_prefix='sentence-transformers-model')\n",
    "print(f\"Response from model upload: {inputs}\") \n",
    "\n",
    "# Create a SageMaker session and get the execution role to be used later \n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Deploy with model data \n",
    "hub = {\n",
    "    'HF_TASK':'feature-extraction'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=inputs,  # path to your trained SageMaker model\n",
    "   role=role,                                            # IAM role with permissions to create an endpoint\n",
    "   transformers_version=\"4.26\",                           # Transformers version used\n",
    "   pytorch_version=\"1.13\",                                # PyTorch version used\n",
    "   py_version='py39',                                    # Python version used\n",
    "   env=hub\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.t2.medium\",\n",
    "   endpoint_name = f'all-mpnet-base-v2-mpf-huggingface-test'\n",
    ")\n",
    "\n",
    "# example request: you always need to define \"inputs\"\n",
    "data = {\"inputs\":\" Today is a sunny and nice day in Ottawa\"} \n",
    "# request\n",
    "vector = predictor.predict(data)\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551515bb-f6cb-4b24-8616-82e89a44779e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Test the model endpoints and perform search in OpenSearch index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7953749c-5d48-4ec8-91ee-c8c7fb98dd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker_fn import invoke_sagemaker_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec9e3196-fb03-489b-b913-b584fbf13632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "endpoint_name ='all-mpnet-base-v2-mpf-huggingface-test'\n",
    "payload = {\"inputs\": \"floods event in Canada\"}\n",
    "vector = invoke_sagemaker_endpoint(endpoint_name, payload)\n",
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "144f2de9-7e98-423d-8417-1859b1339224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to OpenSearch established: <OpenSearch([{'host': 'search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com', 'port': 443}])>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>title</th>\n",
       "      <th>uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15eQT5ABdooaaHUe8g0S</td>\n",
       "      <td>0.727605</td>\n",
       "      <td>Floods in Canada - Cartographic Product Collec...</td>\n",
       "      <td>08b810c2-7c81-40f1-adb1-c32c8a2c9f50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65eST5ABdooaaHUeTiL6</td>\n",
       "      <td>0.664807</td>\n",
       "      <td>High tides December 2010: breaking waves</td>\n",
       "      <td>39bdcc75-dbaf-424d-9dbd-265c282f14f5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GZeST5ABdooaaHUeZCRK</td>\n",
       "      <td>0.653705</td>\n",
       "      <td>CGDIWH-142543</td>\n",
       "      <td>CGDIWH-142543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_ZeRT5ABdooaaHUeixbR</td>\n",
       "      <td>0.643081</td>\n",
       "      <td>Flood Risk Areas Database (BDZI)</td>\n",
       "      <td>3ac8ddff-fe0a-4a7a-8393-d5938e8f35e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bJeRT5ABdooaaHUeghbl</td>\n",
       "      <td>0.637360</td>\n",
       "      <td>Flooding zones</td>\n",
       "      <td>12c51ab4-e22a-4abd-bf90-eaeb274a98c9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C5eRT5ABdooaaHUe-B5j</td>\n",
       "      <td>0.624253</td>\n",
       "      <td>Flood Risk Areas and Historical Floods</td>\n",
       "      <td>35782937-d7ac-b721-7fb3-bf51f18903ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QJeRT5ABdooaaHUezBvt</td>\n",
       "      <td>0.623981</td>\n",
       "      <td>Forest Abiotic Damage Event</td>\n",
       "      <td>c32dfe71-bb89-4301-a8a3-4f97d1629c00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35eTT5ABdooaaHUeeTT-</td>\n",
       "      <td>0.621082</td>\n",
       "      <td>2023 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-compilation-dsw-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WpeST5ABdooaaHUe5ywG</td>\n",
       "      <td>0.620454</td>\n",
       "      <td>Government of Qc - 2019 Flood</td>\n",
       "      <td>CGDIWH-117987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gZeST5ABdooaaHUeaiRp</td>\n",
       "      <td>0.620081</td>\n",
       "      <td>Flood_Inondation_EGS_Flood_Product_Active_en</td>\n",
       "      <td>CGDIWH-150532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3peTT5ABdooaaHUeeTTt</td>\n",
       "      <td>0.620029</td>\n",
       "      <td>2021 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-compilation-dsw-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tZeST5ABdooaaHUeqyhn</td>\n",
       "      <td>0.619563</td>\n",
       "      <td>Flood zones</td>\n",
       "      <td>c4c9f1d0-85ce-479f-b18f-7fc5b84de024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LpeST5ABdooaaHUeoyhg</td>\n",
       "      <td>0.618755</td>\n",
       "      <td>Collection - Dynamic Surface Water Maps of Can...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-compilation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L5eST5ABdooaaHUeoyht</td>\n",
       "      <td>0.618482</td>\n",
       "      <td>2019 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-compilation-dsw-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9peST5ABdooaaHUeoCcn</td>\n",
       "      <td>0.618184</td>\n",
       "      <td>2011 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-annual-dsw-2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CJeST5ABdooaaHUeoSgK</td>\n",
       "      <td>0.618074</td>\n",
       "      <td>1992 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-annual-dsw-1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-5eST5ABdooaaHUeoCdn</td>\n",
       "      <td>0.617953</td>\n",
       "      <td>2005 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-annual-dsw-2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DJeTT5ABdooaaHUeSTII</td>\n",
       "      <td>0.617791</td>\n",
       "      <td>CGDIWH-44523</td>\n",
       "      <td>CGDIWH-44523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mZeST5ABdooaaHUe-S0q</td>\n",
       "      <td>0.617778</td>\n",
       "      <td>Floods in Canada - Archive</td>\n",
       "      <td>CGDIWH-150530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-peST5ABdooaaHUeoCdZ</td>\n",
       "      <td>0.617715</td>\n",
       "      <td>2006 - Dynamic Surface Water Maps of Canada fr...</td>\n",
       "      <td>ccmeo-dynamic-surface-water-annual-dsw-2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     _id  relevancy_score  \\\n",
       "0   15eQT5ABdooaaHUe8g0S         0.727605   \n",
       "1   65eST5ABdooaaHUeTiL6         0.664807   \n",
       "2   GZeST5ABdooaaHUeZCRK         0.653705   \n",
       "3   _ZeRT5ABdooaaHUeixbR         0.643081   \n",
       "4   bJeRT5ABdooaaHUeghbl         0.637360   \n",
       "5   C5eRT5ABdooaaHUe-B5j         0.624253   \n",
       "6   QJeRT5ABdooaaHUezBvt         0.623981   \n",
       "7   35eTT5ABdooaaHUeeTT-         0.621082   \n",
       "8   WpeST5ABdooaaHUe5ywG         0.620454   \n",
       "9   gZeST5ABdooaaHUeaiRp         0.620081   \n",
       "10  3peTT5ABdooaaHUeeTTt         0.620029   \n",
       "11  tZeST5ABdooaaHUeqyhn         0.619563   \n",
       "12  LpeST5ABdooaaHUeoyhg         0.618755   \n",
       "13  L5eST5ABdooaaHUeoyht         0.618482   \n",
       "14  9peST5ABdooaaHUeoCcn         0.618184   \n",
       "15  CJeST5ABdooaaHUeoSgK         0.618074   \n",
       "16  -5eST5ABdooaaHUeoCdn         0.617953   \n",
       "17  DJeTT5ABdooaaHUeSTII         0.617791   \n",
       "18  mZeST5ABdooaaHUe-S0q         0.617778   \n",
       "19  -peST5ABdooaaHUeoCdZ         0.617715   \n",
       "\n",
       "                                                title  \\\n",
       "0   Floods in Canada - Cartographic Product Collec...   \n",
       "1            High tides December 2010: breaking waves   \n",
       "2                                       CGDIWH-142543   \n",
       "3                    Flood Risk Areas Database (BDZI)   \n",
       "4                                      Flooding zones   \n",
       "5              Flood Risk Areas and Historical Floods   \n",
       "6                         Forest Abiotic Damage Event   \n",
       "7   2023 - Dynamic Surface Water Maps of Canada fr...   \n",
       "8                       Government of Qc - 2019 Flood   \n",
       "9        Flood_Inondation_EGS_Flood_Product_Active_en   \n",
       "10  2021 - Dynamic Surface Water Maps of Canada fr...   \n",
       "11                                        Flood zones   \n",
       "12  Collection - Dynamic Surface Water Maps of Can...   \n",
       "13  2019 - Dynamic Surface Water Maps of Canada fr...   \n",
       "14  2011 - Dynamic Surface Water Maps of Canada fr...   \n",
       "15  1992 - Dynamic Surface Water Maps of Canada fr...   \n",
       "16  2005 - Dynamic Surface Water Maps of Canada fr...   \n",
       "17                                       CGDIWH-44523   \n",
       "18                         Floods in Canada - Archive   \n",
       "19  2006 - Dynamic Surface Water Maps of Canada fr...   \n",
       "\n",
       "                                                 uuid  \n",
       "0                08b810c2-7c81-40f1-adb1-c32c8a2c9f50  \n",
       "1                39bdcc75-dbaf-424d-9dbd-265c282f14f5  \n",
       "2                                       CGDIWH-142543  \n",
       "3                3ac8ddff-fe0a-4a7a-8393-d5938e8f35e5  \n",
       "4                12c51ab4-e22a-4abd-bf90-eaeb274a98c9  \n",
       "5                35782937-d7ac-b721-7fb3-bf51f18903ba  \n",
       "6                c32dfe71-bb89-4301-a8a3-4f97d1629c00  \n",
       "7   ccmeo-dynamic-surface-water-compilation-dsw-19...  \n",
       "8                                       CGDIWH-117987  \n",
       "9                                       CGDIWH-150532  \n",
       "10  ccmeo-dynamic-surface-water-compilation-dsw-19...  \n",
       "11               c4c9f1d0-85ce-479f-b18f-7fc5b84de024  \n",
       "12            ccmeo-dynamic-surface-water-compilation  \n",
       "13  ccmeo-dynamic-surface-water-compilation-dsw-19...  \n",
       "14        ccmeo-dynamic-surface-water-annual-dsw-2011  \n",
       "15        ccmeo-dynamic-surface-water-annual-dsw-1992  \n",
       "16        ccmeo-dynamic-surface-water-annual-dsw-2005  \n",
       "17                                       CGDIWH-44523  \n",
       "18                                      CGDIWH-150530  \n",
       "19        ccmeo-dynamic-surface-water-annual-dsw-2006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region = \"ca-central-1\"\n",
    "aos_host = \"search-semantic-search-dfcizxxxuj62dusl5skmeu3czu.ca-central-1.es.amazonaws.com\"\n",
    "os_secret_id = \"dev/OpenSearch/SemanticSearch\"\n",
    "\n",
    "awsauth = get_awsauth_from_secret(region, secret_id=os_secret_id)\n",
    "aos_client =create_opensearch_connection(aos_host, awsauth)\n",
    "\n",
    "query={\n",
    "    \"size\": 20,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"vector\":{\n",
    "                \"vector\":vector,\n",
    "                \"k\":20\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index='mpnet-mpf-knn', size=20, body=query, request_timeout=55)\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['_source']['title'],hit['_source']['id']]\n",
    "    query_result.append(row)\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"relevancy_score\",\"title\",'uuid'])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51722f-3991-41e3-aae5-3cbdfbec49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
