{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5889142c",
   "metadata": {},
   "source": [
    "# Semantic Search with Amazon OpenSearch Service \n",
    "To create semantic search, we will add a vector representation of the metadata to our data set in OpenSearch, then do the same with our sample query \"Wildfires in Canada\". In OpenSearch, we'll use a KNN search to find matches based on a cosine similarity rating on the vector.\n",
    "We will:\n",
    "1. Use a HuggingFace sentence-transformer BERT model to generate sentence embedding for the geo.ca metadata dataset\n",
    "2. Upload the dataset to OpenSearch, with the original metadata schema text combined with the vector representation of the questions.\n",
    "3. Translate the query question to a vector.\n",
    "4. Perform a KNN search in OpenSearch to perform semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384cbf1",
   "metadata": {},
   "source": [
    "### 1. Check PyTorch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3494a2",
   "metadata": {},
   "source": [
    "As in the previous modules, let's import PyTorch and confirm that have have the latest version of PyTorch. The version should already be 2.0.1 or higher. If not, please run the lab in order to get everything set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12fd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only run the line below to install the latest torch. -kernel restart needed\n",
    "#!pip install --upgrade torch torchvision torchaudio\n",
    "\n",
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78e2e6",
   "metadata": {},
   "source": [
    "### 2. Retrieve notebook variables\n",
    "\n",
    "The line below will retrieve your shared variables from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061e933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9989253",
   "metadata": {},
   "source": [
    "### 3. import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91183382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #installed in the previous notebook\n",
    "!pip install -q boto3\n",
    "!pip install -q requests\n",
    "!pip install -q requests-aws4auth\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm\n",
    "!pip install -q boto3\n",
    "!pip install -q transformers[torch]\n",
    "!pip install -q transformers\n",
    "!pip install -q sentence-transformers rank_bm25\n",
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9198016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fc037",
   "metadata": {},
   "source": [
    "### 4. Prepare BERT Model \n",
    "#### Option 1: DistilBert model\n",
    "For this module, we will be using the HuggingFace BERT model to generate vectorization data, where every sentence is 768 dimension data. Let's create some helper functions we'll use later on.\n",
    "![BERT](image/nlp_bert.png)\n",
    "\n",
    "We are creating 2 functions:\n",
    "1. mean_pooling\n",
    "2. sentence_to_vector - this is the key function we'll use to generate our vector embedding for the metadata dataset.\n",
    "\n",
    "A reason for not using DistilBert:\n",
    " Transformer models like DistilBert have a fixed maximum input length (512), and any input longer than this limit can cause errors during processing.Our input sequence length (1086 tokens) exceeds the model's maximum sequence length (512 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# #model_name = \"distilbert-base-uncased\"\n",
    "# #model_name = \"sentence-transformers/msmarco-distilbert-base-dot-prod-v3\"\n",
    "# model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\" #https://huggingface.co/sentence-transformers/distilbert-base-nli-stsb-mean-tokens\n",
    "\n",
    "\n",
    "# #Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "#     sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "#     return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "# def sentence_to_vector(raw_inputs):\n",
    "#     tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "#     model = DistilBertModel.from_pretrained(model_name)\n",
    "#     inputs_tokens = tokenizer(raw_inputs, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs_tokens)\n",
    "\n",
    "#     sentence_embeddings = mean_pooling(outputs, inputs_tokens['attention_mask'])\n",
    "#     return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd51625",
   "metadata": {},
   "source": [
    "#### Option 2: all-MiniLM-L6-v2\n",
    "We can also use sentence-transformer models ['all-MiniLM-L6-v2'](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which simplifies the process of obtaining sentence embeddings. It has 384 dimensions.It designed for generating sentence embeddings directly, which means we can use the Sentence Transformers library's functionality to handle both tokenization and embedding in a more streamlined manner compared to manually handling with DistilBertModel and DistilBertTokenizer.\n",
    "\n",
    "The SentenceTransformer class's encode method directly handles the text input, tokenization, and conversion to sentence embeddings, eliminating the need for manual mean pooling. The encode method returns a tensor of sentence embeddings, where each embedding corresponds to the input sentences provided to the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92559020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np \n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "model.to(device)  # ðŸ‘ˆ move model to GPU or CPU\n",
    "\n",
    "def sentence_to_vector(raw_inputs):\n",
    "    # Encode sentences to get sentence embeddings\n",
    "    sentence_embeddings = model.encode(raw_inputs, convert_to_tensor=True)\n",
    "    \"\"\"\n",
    "    When you work with vectors (such as embeddings) in Elasticsearch or OpenSearch, you need to convert the PyTorch tensor to a list of floats before indexing the document. \n",
    "    \"\"\"\n",
    "    encod_np_array = np.array(sentence_embeddings)\n",
    "    encod_list = encod_np_array.tolist()\n",
    "        \n",
    "    return encod_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8ddf3",
   "metadata": {},
   "source": [
    "### 5.Preprocess and embed the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11644ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import boto3\n",
    "import json\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import io\n",
    "import os \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0531c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "def read_parquet_from_s3_as_df(region, s3_bucket, s3_key):\n",
    "    \"\"\"\n",
    "    Load a Parquet file from an S3 bucket into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - region: AWS region where the S3 bucket is located.\n",
    "    - s3_bucket: Name of the S3 bucket.\n",
    "    - s3_key: Key (path) to the Parquet file within the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame containing the data from the Parquet file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup AWS session and clients\n",
    "    session = boto3.Session(region_name=region)\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # Load the Parquet file as a pandas DataFrame\n",
    "    object = s3.Object(s3_bucket, s3_key)\n",
    "    body = object.get()['Body'].read()\n",
    "    df = pd.read_parquet(io.BytesIO(body))\n",
    "    return df\n",
    "\n",
    "# Upload the duplicate date to S3 as a parquet file \n",
    "def upload_df_to_s3_as_parquet(df, bucket_name, file_key):\n",
    "    # Save DataFrame as a Parquet file locally\n",
    "    parquet_file_path = 'temp.parquet'\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Upload the Parquet file to S3 bucket\n",
    "    try:\n",
    "        response = s3_client.upload_file(parquet_file_path, bucket_name, file_key)\n",
    "        os.remove(parquet_file_path)\n",
    "        print(f'Uploading {file_key} to {bucket_name} as parquet file')\n",
    "        # Delete the local Parquet file\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "# Create new column 'organization_en' required by the API JSON response \n",
    "def extract_organisation_en(contact_str):\n",
    "    try:\n",
    "        # Parse the stringified JSON into Python objects\n",
    "        contact_data = json.loads(contact_str)\n",
    "        # If the parsed data is a list, iterate through it\n",
    "        if isinstance(contact_data, list):\n",
    "            for item in contact_data:\n",
    "                # Check if 'organisation' and 'en' keys exist\n",
    "                if 'organisation' in item and 'en' in item['organisation']:\n",
    "                    return item['organisation']['en']\n",
    "        elif isinstance(contact_data, dict):\n",
    "            # If the data is a dictionary, extract 'organisation' in 'en' directly\n",
    "            return contact_data.get('organisation', {}).get('en', None)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle cases where the contact string is not valid JSON\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other unexpected errors\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Text preprocess\n",
    "def preprocess_records_into_text(df):\n",
    "    selected_columns = ['features_properties_title_en','features_properties_description_en','features_properties_keywords_en']\n",
    "    df = df[selected_columns]\n",
    "    return df.apply(lambda x: f\"{x['features_properties_title_en']}\\n{x['features_properties_description_en']}\\nkeywords:{x['features_properties_keywords_en']}\",axis=1 )\n",
    "\n",
    "\n",
    "# Text preprocess\n",
    "def preprocess_records_into_text_multi(df):\n",
    "    selected_columns = ['features_properties_title_en','features_properties_description_en','features_properties_keywords_en','features_properties_title_fr','features_properties_description_fr','features_properties_keywords_fr']\n",
    "    df = df[selected_columns]\n",
    "    return df.apply(lambda x: f\"{x['features_properties_title_en']}\\n{x['features_properties_title_fr']}\\n{x['features_properties_description_en']}\\n{x['features_properties_description_fr']}\\nkeywords:{x['features_properties_keywords_en']}\\nkeywords:{x['features_properties_keywords_fr']}\",axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c78b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1) Step1: Load the data \n",
    "df_parquet = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-geocore-geojson-to-parquet-prod', 'records.parquet')\n",
    "df_sentinel1_1 = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-geocore-geojson-to-parquet-prod', '1-sentinel1.parquet')\n",
    "df_sentinel1_2 = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-geocore-geojson-to-parquet-prod', '2-sentinel1.parquet')\n",
    "df_rcm = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-geocore-geojson-to-parquet-prod', 'rcm-ard.parquet')\n",
    "df = pd.concat([df_parquet, df_sentinel1_1, df_sentinel1_2, df_rcm], ignore_index=True)\n",
    "#df = pd.concat([df_parquet, df_rcm], ignore_index=True)\n",
    "df.head()\n",
    "#df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1b73b",
   "metadata": {},
   "source": [
    "Subset to columns that are required in the app.geo.ca [api response](https://geocore.api.geo.ca/geo?north=81.77364370720657&east=360&south=-8.407168163601076&west=-359.6484375&keyword=&lang=en&min=1&max=10&sort=popularity-desc). \n",
    "##### Note, we are focus on the english search at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04376a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2) Step2: Clean the data  \n",
    "col_names_list = [\n",
    "    'features_properties_id','features_geometry_coordinates','features_properties_title_en',\n",
    "    'features_properties_description_en','features_properties_date_published_date',\n",
    "    'features_properties_keywords_en','features_properties_options','features_properties_contact',\n",
    "    'features_properties_topicCategory','features_properties_date_created_date',\n",
    "    'features_properties_spatialRepresentation','features_properties_type',\n",
    "    'features_properties_temporalExtent_begin','features_properties_temporalExtent_end',\n",
    "    'features_properties_graphicOverview','features_properties_language','features_popularity',\n",
    "    'features_properties_sourceSystemName','features_properties_eoCollection',\n",
    "    'features_properties_eoFilters'\n",
    "]\n",
    "col_names_list_multi = [\n",
    "    'features_properties_id','features_geometry_coordinates','features_properties_title_en','features_properties_title_fr',\n",
    "    'features_properties_description_en','features_properties_description_fr','features_properties_date_published_date',\n",
    "    'features_properties_keywords_en','features_properties_keywords_fr','features_properties_options','features_properties_contact','features_properties_cited',\n",
    "    'features_properties_topicCategory','features_properties_date_created_date',\n",
    "    'features_properties_spatialRepresentation','features_properties_type',\n",
    "    'features_properties_temporalExtent_begin','features_properties_temporalExtent_end',\n",
    "    'features_properties_graphicOverview','features_properties_language','features_popularity',\n",
    "    'features_properties_sourceSystemName','features_properties_eoCollection',\n",
    "    'features_properties_eoFilters'\n",
    "]\n",
    "df_en = df[col_names_list_multi]\n",
    "df_en['organisation_en'] = df_en['features_properties_contact'].apply(extract_organisation_en)\n",
    "df_en['organisation_en_cited'] = df_en['features_properties_cited'].apply(extract_organisation_en)\n",
    "\n",
    "# Create a new column 'temporalExtent' as a dictionary of {'begin': ..., 'end': ...}\n",
    "values_to_replace = {'Present': None, 'Not Available; Indisponible': None}\n",
    "columns_to_replace = ['features_properties_temporalExtent_begin', 'features_properties_temporalExtent_end']\n",
    "df_en[columns_to_replace] = df_en[columns_to_replace].replace(values_to_replace)\n",
    "\n",
    "df_en['temporalExtent'] = df_en.apply(lambda row: {'begin': row['features_properties_temporalExtent_begin'], 'end': row['features_properties_temporalExtent_end']}, axis=1)\n",
    "df_en = df_en.drop(columns =['features_properties_temporalExtent_begin', 'features_properties_temporalExtent_end'])\n",
    "\n",
    "values_to_replace = {'Not Available; Indisponible': None} # modifies dates to acceptable values\n",
    "columns_to_replace = ['features_properties_date_published_date', 'features_properties_date_created_date']\n",
    "df_en[columns_to_replace] = df_en[columns_to_replace].replace(values_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b2fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3) Step 3: Preprocess text \n",
    "#df_en['text'] = preprocess_records_into_text(df_en)\n",
    "df_en['text'] = preprocess_records_into_text_multi(df_en)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_en.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dd906",
   "metadata": {},
   "source": [
    "### (optional) Text Preprocess using NLTK  \n",
    "\n",
    "Create a new column that concadenate the selected columns: features_properties_title_en, features_properties_description_en,features_properties_keywords_en, and apply the following preprocesing before tokenization:\n",
    "- convert to lower case \n",
    "- remove stopwords and punctuation\n",
    "- remove apostrophe\n",
    "- stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258c36f-b4ba-4e54-aec7-a8b03143142c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import word_tokenize   # module for tokenizing strings \n",
    "import string\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "df_en['metadata_en'] = df_en['features_properties_title_en'] + ' ' + df_en['features_properties_description_en'] + ' ' + df_en['features_properties_keywords_en'] + ' ' + df_en['features_properties_title_fr'] + ' ' + df_en['features_properties_description_fr'] + ' ' + df_en['features_properties_keywords_fr']\n",
    "if df_en['metadata_en'].isnull().any():\n",
    "    df_en['metadata_en'] = df_en['metadata_en'].fillna('')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "#     \"\"\"\n",
    "#     text: raw tex input, a string of text, or a list of string\n",
    "#     output: preprocess text in string format \n",
    "#     \"\"\"\n",
    "#     # Set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "#     # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "#     # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation.replace(\"'\", \"\")))  # Keep apostrophe\n",
    "#     # Remove apostrophes\n",
    "    text = text.replace(\"'\", \"\")\n",
    "#     # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "#     # Remove stopwords and stem\n",
    "    filtered_text = [stemmer.stem(word) for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "df_en['processed_metadata_en'] = df_en['metadata_en'].apply(clean_text)\n",
    "# # Show the processed column\n",
    "print(df_en.loc[1:3, ['metadata_en', 'processed_metadata_en']])\n",
    "df_en.head(4)\n",
    "\n",
    "tqdm.pandas()\n",
    "df_en['vector'] = df_en[\"processed_metadata_en\"].progress_apply(sentence_to_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fb697-37f2-4c9c-8e11-cb4396e38cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Embedding text \n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "df_en['vector'] = df_en[\"text\"].progress_apply(sentence_to_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3d02f-7405-4502-9608-3ba66a3a0886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The dimension of the vector is {len(df_en.loc[1, \"vector\"])}')\n",
    "df_en.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0edff0-53ab-4869-a50d-a96649f6fa49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5 Upload the embeddings as a parquet file to S3 bucket \n",
    "upload_df_to_s3_as_parquet(df=df_en, bucket_name='webpresence-nlp-data-preprocessing-prod', file_key='semantic_search_embeddings-minilm-multilingual.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7713302",
   "metadata": {},
   "source": [
    "### 6. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a4127-e439-4041-b7ff-430a7fb6feca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearch import get_awsauth_from_secret, create_opensearch_connection, delete_aos_index_if_exists, load_data_to_opensearch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30507c-8c39-4221-834d-e8f905a861fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "def read_parquet_from_s3_as_df(region, s3_bucket, s3_key):\n",
    "    \"\"\"\n",
    "    Load a Parquet file from an S3 bucket into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - region: AWS region where the S3 bucket is located.\n",
    "    - s3_bucket: Name of the S3 bucket.\n",
    "    - s3_key: Key (path) to the Parquet file within the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame containing the data from the Parquet file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup AWS session and clients\n",
    "    session = boto3.Session(region_name=region)\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # Load the Parquet file as a pandas DataFrame\n",
    "    object = s3.Object(s3_bucket, s3_key)\n",
    "    body = object.get()['Body'].read()\n",
    "    df = pd.read_parquet(io.BytesIO(body))\n",
    "    return df\n",
    "\n",
    "#Optional: read the embedding data from the S3 bucket \n",
    "import pandas as pd \n",
    "import io\n",
    "df_en = read_parquet_from_s3_as_df('ca-central-1', 'webpresence-nlp-data-preprocessing-prod', 'semantic_search_embeddings-minilm-multilingual.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae40e1-7ed7-4d0b-ad93-a642014f7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_en[df_en['features_properties_id'] == '0a2e22a2-a7a1-47d4-9249-3eaafe1c815b']\n",
    "row\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_en.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4085105-d656-46b5-95cd-a199ffdb1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "\n",
    "# 1. Change cgp to geo-ca for source-system\n",
    "df_en.loc[\n",
    "    (df_en['features_properties_sourceSystemName'] == 'cgp') |\n",
    "    (df_en['features_properties_sourceSystemName'].isna()),\n",
    "    'features_properties_sourceSystemName'\n",
    "] = 'geo-ca'\n",
    "\n",
    "# 2. Create a new column called features_properties_mappable and set to true if options protocol sectoion contains a wildcard of 'esri' or 'ogc'\n",
    "def is_mappable_from_str(options_str):\n",
    "    try:\n",
    "        import json\n",
    "        options = json.loads(options_str) if isinstance(options_str, str) else options_str\n",
    "        \n",
    "        protocols = []\n",
    "        if isinstance(options, list):\n",
    "            protocols = [opt.get(\"protocol\", \"\").strip().lower() for opt in options if isinstance(opt, dict)]\n",
    "        elif isinstance(options, dict):\n",
    "            protocol = options.get(\"protocol\", \"\").strip().lower()\n",
    "            protocols = [protocol]\n",
    "                \n",
    "        return any(\"esri\" in p or \"ogc\" in p for p in protocols)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing options: {e}\")\n",
    "        return False\n",
    "\n",
    "df_en[\"features_properties_mappable\"] = df_en[\"features_properties_options\"].apply(is_mappable_from_str)\n",
    "\n",
    "df_en[\"features_properties_mappable\"] = df_en[\"features_properties_mappable\"].astype(str).str.lower()\n",
    "\n",
    "\n",
    "# 3. Create a new column called features_properties_geo_theme and bin ISO topic categories\n",
    "\n",
    "theme_bins = {\n",
    "    'boundaries': 'administration',\n",
    "    'planningcadastre': 'administration',\n",
    "    'location': 'administration',\n",
    "    'transportation': 'administration',\n",
    "\n",
    "    'economy': 'economy',\n",
    "    'farming': 'economy',\n",
    "\n",
    "    'biota': 'environment',\n",
    "    'environment': 'environment',\n",
    "    'elevation': 'environment',\n",
    "    'inlandwaters': 'environment',\n",
    "    'oceans': 'environment',\n",
    "    'climatologymeteorologyatmosphere': 'environment',  # lowercase key\n",
    "\n",
    "    'imagerybasemapsearthcover': 'imagery',\n",
    "    'earthobservation;syntheticaperatureradar': 'imagery',\n",
    "\n",
    "    'structure': 'infrastructure',\n",
    "    'transport': 'infrastructure',\n",
    "    'utilitiescommunication': 'infrastructure',\n",
    "\n",
    "    'geoscientificinformation': 'science',  # lowercase key\n",
    "\n",
    "    'health': 'society',\n",
    "    'society': 'society',\n",
    "    'intelligencemilitary': 'society',\n",
    "}\n",
    "\n",
    "def map_topics_to_themes(topic_str):\n",
    "    if not isinstance(topic_str, str):\n",
    "        return []\n",
    "\n",
    "    topics = [t.strip().lower() for t in topic_str.split(\",\")]\n",
    "    themes = {theme_bins.get(topic) for topic in topics if theme_bins.get(topic)}\n",
    "\n",
    "    # Convert everything to strings explicitly (just in case)\n",
    "    return sorted([str(theme) for theme in themes])\n",
    "\n",
    "df_en[\"features_properties_geo_theme\"] = df_en[\"features_properties_topicCategory\"].apply(map_topics_to_themes)\n",
    "\n",
    "#print(df_en[\"features_properties_geo_theme\"].iloc[0])\n",
    "# should output: [\"administration\", \"society\"]\n",
    "\n",
    "#print(type(df_en[\"features_properties_geo_theme\"].iloc[0][0]))\n",
    "# should output: <class 'str'>\n",
    "\n",
    "\n",
    "# 4. Organizations should not have divisions\n",
    "def get_second_segment(s):\n",
    "    \"\"\"Extract second segment (index 1) from semicolon-separated string.\"\"\"\n",
    "    if isinstance(s, str):\n",
    "        parts = [p.strip() for p in s.split(\";\")]\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1]\n",
    "    return None\n",
    "\n",
    "def extract_org(contact_list):\n",
    "    \"\"\"Extract org dict with 'en' and 'fr' second segments.\"\"\"\n",
    "    if isinstance(contact_list, str):\n",
    "        try:\n",
    "            contact_list = json.loads(contact_list)\n",
    "        except Exception:\n",
    "            return {\"en\": None, \"fr\": None}\n",
    "\n",
    "    if not isinstance(contact_list, list) or not contact_list:\n",
    "        return {\"en\": None, \"fr\": None}\n",
    "\n",
    "    # filter out empty/nulls\n",
    "    contact_list = [c for c in contact_list if isinstance(c, dict) and c]\n",
    "    if not contact_list:\n",
    "        return {\"en\": None, \"fr\": None}\n",
    "\n",
    "    org = contact_list[0].get(\"organisation\", {})\n",
    "    if not isinstance(org, dict):\n",
    "        return {\"en\": None, \"fr\": None}\n",
    "\n",
    "    return {\n",
    "        \"en\": get_second_segment(org.get(\"en\")),\n",
    "        \"fr\": get_second_segment(org.get(\"fr\"))\n",
    "    }\n",
    "\n",
    "def choose_org(cited, contact):\n",
    "    \"\"\"Use cited if valid, else fallback to contact.\"\"\"\n",
    "    if cited in [None, [], {}, [None], \"[]\", \"[null]\"] or (\n",
    "        isinstance(cited, float) and pd.isna(cited)\n",
    "    ):\n",
    "        return extract_org(contact)\n",
    "    return extract_org(cited)\n",
    "\n",
    "# âš¡ Faster: use vectorized row-wise operation only ONCE\n",
    "df_en[\"features_properties_org\"] = [\n",
    "    choose_org(cited, contact)\n",
    "    for cited, contact in zip(df_en[\"features_properties_cited\"], df_en[\"features_properties_contact\"])\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# 5. ISO themes should be an array\n",
    "df_en['features_properties_topicCategory'] = df_en['features_properties_topicCategory'].apply(\n",
    "    lambda x: [s.strip() for s in x.split(',')] if isinstance(x, str) else x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "df_en[\"features_properties_keywords_en\"] = df_en[\"features_properties_keywords_en\"].apply(\n",
    "    lambda x: [s.strip() for s in x.split(',')] if isinstance(x, str) else x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "df_en[\"features_properties_keywords_fr\"] = df_en[\"features_properties_keywords_fr\"].apply(\n",
    "    lambda x: [s.strip() for s in x.split(',')] if isinstance(x, str) else x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Extract unique English descriptions (before first semicolon) across all options\n",
    "\n",
    "def extract_unique_eng_desc(options_str):\n",
    "    try:\n",
    "        import json\n",
    "        options = json.loads(options_str) if isinstance(options_str, str) else options_str\n",
    "\n",
    "        descs = []\n",
    "        if isinstance(options, list):\n",
    "            for opt in options:\n",
    "                if isinstance(opt, dict):\n",
    "                    desc = opt.get(\"description\", {}).get(\"en\")\n",
    "                    if isinstance(desc, str):\n",
    "                        # keep only text before first semicolon\n",
    "                        descs.append(desc.split(\";\")[0].strip())\n",
    "        elif isinstance(options, dict):\n",
    "            desc = options.get(\"description\", {}).get(\"en\")\n",
    "            if isinstance(desc, str):\n",
    "                descs.append(desc.split(\";\")[0].strip())\n",
    "\n",
    "        return list(set(descs))  # unique\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing options: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "df_en[\"features_properties_type\"] = df_en[\"features_properties_options\"].apply(extract_unique_eng_desc)\n",
    "\n",
    "\n",
    "# Find rows where the theme column is an empty list ([])\n",
    "#empty_theme_df = df_en[df_en[\"features_properties_geo_theme\"].apply(lambda x: isinstance(x, list) and len(x) == 0)]\n",
    "# Display the filtered DataFrame\n",
    "#empty_theme_df\n",
    "\n",
    "row = df_en[df_en['features_properties_id'] == '0a2e22a2-a7a1-47d4-9249-3eaafe1c815b']\n",
    "row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d152847-1dbe-4d5b-8987-6a07229e00e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data normalization\n",
    "\n",
    "#1 Change cgp to geo-ca for source-system\n",
    "df_en.loc[\n",
    "    (df_en['features_properties_sourceSystemName'] == 'cgp') |\n",
    "    (df_en['features_properties_sourceSystemName'].isna()),\n",
    "    'features_properties_sourceSystemName'\n",
    "] = 'geo-ca'\n",
    "\n",
    "#2 Create a new column called features_properties_mappable and set to true if options protocol sectoion contains a wildcard of 'esri' or 'ogc'\n",
    "def is_mappable_from_str(options_str):\n",
    "    try:\n",
    "        import json\n",
    "        options = json.loads(options_str) if isinstance(options_str, str) else options_str\n",
    "        \n",
    "        protocols = []\n",
    "        if isinstance(options, list):\n",
    "            protocols = [opt.get(\"protocol\", \"\").strip().lower() for opt in options if isinstance(opt, dict)]\n",
    "        elif isinstance(options, dict):\n",
    "            protocol = options.get(\"protocol\", \"\").strip().lower()\n",
    "            protocols = [protocol]       \n",
    "        return any(\"esri\" in p or \"ogc\" in p for p in protocols)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing options: {e}\")\n",
    "        return False\n",
    "\n",
    "df_en[\"features_properties_mappable\"] = df_en[\"features_properties_options\"].apply(is_mappable_from_str)\n",
    "df_en[\"features_properties_mappable\"] = df_en[\"features_properties_mappable\"].astype(str).str.lower()\n",
    "\n",
    "\n",
    "#4\n",
    "\n",
    "def map_special_topic_category(value):\n",
    "    if isinstance(value, str):\n",
    "        cleaned = value.replace(\" \", \"\").lower()\n",
    "        if cleaned == \"earthobservation;syntheticaperatureradar\":\n",
    "            return \"imageryBaseMapsEarthCover\"\n",
    "    return value\n",
    "\n",
    "df_en[\"features_properties_topicCategory\"] = df_en[\"features_properties_topicCategory\"].apply(map_special_topic_category)\n",
    "\n",
    "# Create a new column called features_properties_geo_theme and bin ISO topic categories\n",
    "theme_bins = {\n",
    "    'boundaries': 'administration',\n",
    "    'planning_cadastre': 'administration',\n",
    "    'location': 'administration',\n",
    "    'transportation': 'administration',\n",
    "    'planningcadastre': 'administration', #missing under old implementation\n",
    "\n",
    "    'economy': 'economy',\n",
    "    'farming': 'economy',\n",
    "\n",
    "    'biota': 'environment',\n",
    "    'environment': 'environment',\n",
    "    'elevation': 'environment',\n",
    "    'inlandwaters': 'environment',\n",
    "    'oceans': 'environment',\n",
    "    'climatologymeteorologyatmosphere': 'environment',  # lowercase key\n",
    "\n",
    "    'imagerybasemapsearthcover': 'imagery',\n",
    "\n",
    "    'structure': 'infrastructure',\n",
    "    'transport': 'infrastructure',\n",
    "    'utilitiescommunication': 'infrastructure',\n",
    "\n",
    "    'geoscientificinformation': 'science',  # lowercase key\n",
    "\n",
    "    'health': 'society',\n",
    "    'society': 'society',\n",
    "    'intelligencemilitary': 'society',\n",
    "}\n",
    "\n",
    "def map_topics_to_themes(topic_str):\n",
    "    if not isinstance(topic_str, str):\n",
    "        return []\n",
    "\n",
    "    topics = [t.strip().lower() for t in topic_str.split(\",\")]\n",
    "    themes = {theme_bins.get(topic) for topic in topics if theme_bins.get(topic)}\n",
    "\n",
    "    # Convert everything to strings explicitly (just in case)\n",
    "    return sorted([str(theme) for theme in themes])\n",
    "\n",
    "df_en[\"features_properties_geo_theme\"] = df_en[\"features_properties_topicCategory\"].apply(map_topics_to_themes)\n",
    "\n",
    "# 5 Organizations should not have divisions\n",
    "\n",
    "# List of 8 UUIDs to overwrite\n",
    "health_canada_ids = [\n",
    "    \"d256b422-2834-40a2-9f0c-bd5fc32781b2\",\n",
    "    \"12acd145-626a-49eb-b850-0a59c9bc7506\",\n",
    "    \"685257af-5592-49de-8726-90090c6c3061\",\n",
    "    \"16348a20-02f4-4557-b68c-d1754e3026a4\",\n",
    "    \"0f45b62a-b4f6-4eaa-ad84-2fca80108d0b\",\n",
    "    \"67bedee8-beb0-4b3a-a1c6-24a4cda08afe\",\n",
    "    \"f0d1c3a9-cf78-4b07-af55-9e8d4303449e\",\n",
    "    \"21b821cf-0f1c-40ee-8925-eab12d357668\",\n",
    "]\n",
    "\n",
    "hc_org = {\"en\": \"Government of Canada;Health Canada\", \"fr\": \"Gouvernement du Canada;SantÃ© Canada\"}\n",
    "\n",
    "for uid in health_canada_ids:\n",
    "    idx = df_en.index[df_en[\"features_properties_id\"] == uid]\n",
    "    if not idx.empty:\n",
    "        i = idx[0]\n",
    "\n",
    "        contact_list = df_en.at[i, \"features_properties_contact\"]\n",
    "\n",
    "        # Parse stringified JSON if needed\n",
    "        if isinstance(contact_list, str):\n",
    "            try:\n",
    "                contact_list = json.loads(contact_list)\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # Skip if malformed\n",
    "\n",
    "        # Modify organisation field\n",
    "        if isinstance(contact_list, list) and len(contact_list) > 0:\n",
    "            contact_list[0][\"organisation\"] = hc_org\n",
    "            # Re-serialize to JSON string before assigning\n",
    "            df_en.at[i, \"features_properties_contact\"] = json.dumps(contact_list, ensure_ascii=False)\n",
    "            \n",
    "def extract_org_second_segment(contact_list):\n",
    "    try:\n",
    "        # Step 1: If JSON string, parse\n",
    "        if isinstance(contact_list, str):\n",
    "            import json\n",
    "            contact_list = json.loads(contact_list)\n",
    "\n",
    "        # Step 2: Ensure list with at least one contact\n",
    "        if not isinstance(contact_list, list) or not contact_list:\n",
    "            return {\"en\": None, \"fr\": None}\n",
    "\n",
    "        contact = contact_list[0]\n",
    "        org = contact.get(\"organisation\", {})\n",
    "\n",
    "        # Step 3: Extract second segment (index 1) if it exists\n",
    "        def get_second_segment(s):\n",
    "            if isinstance(s, str):\n",
    "                parts = [p.strip() for p in s.split(\";\")]\n",
    "                #print(\"Parts:\", parts)  # Debug print\n",
    "                if len(parts) >= 2:\n",
    "                    return parts[1]\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"en\": get_second_segment(org.get(\"en\")),\n",
    "            \"fr\": get_second_segment(org.get(\"fr\"))\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return {\"en\": None, \"fr\": None}\n",
    "\n",
    "df_en[\"features_properties_org\"] = df_en[\"features_properties_contact\"].apply(extract_org_second_segment)\n",
    "\n",
    "#hc_org = {\"en\": \"Health Canada\", \"fr\": \"SantÃ© Canada\"}\n",
    "\n",
    "#for uid in health_canada_ids:\n",
    "#    df_en.loc[df_en[\"features_properties_id\"] == uid, \"features_properties_org\"] = [hc_org]\n",
    "\n",
    "#6. Themes from dynamodb theme\n",
    "\n",
    "# 6.1. Initialize DynamoDB client\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "table = 'theme'\n",
    "\n",
    "# 6.2. Batch read from DynamoDB (since it is not too large)\n",
    "def scan_table(table_name):\n",
    "    paginator = dynamodb.get_paginator('scan')\n",
    "    items = []\n",
    "    for page in paginator.paginate(TableName=table_name):\n",
    "        items.extend(page['Items'])\n",
    "    return items\n",
    "\n",
    "all_items = scan_table(table)\n",
    "cleaned = [{'uuid': item['uuid']['S'], 'tag': item['tag']['S']} for item in all_items]\n",
    "df_theme = pd.DataFrame(cleaned)\n",
    "\n",
    "# 6.3. Merge with df_en using features_properties_id\n",
    "df_en = df_en.merge(df_theme, how='left', left_on='features_properties_id', right_on='uuid')\n",
    "\n",
    "# 6.4. Append the tag to the geo_theme list if not already present\n",
    "def append_theme(existing, tag):\n",
    "    try:\n",
    "        if not isinstance(existing, list):\n",
    "            existing = []\n",
    "        if isinstance(tag, str) and tag and tag not in existing:\n",
    "            existing.append(tag)\n",
    "        return existing\n",
    "    except Exception:\n",
    "        return existing\n",
    "\n",
    "df_en[\"features_properties_geo_theme\"] = df_en.apply(\n",
    "    lambda row: append_theme(row[\"features_properties_geo_theme\"], row[\"tag\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 6.5. Drop helper columns from step 2\n",
    "df_en.drop(columns=['uuid', 'tag'], inplace=True)\n",
    "\n",
    "# Test\n",
    "#row = df_en[df_en['features_properties_id'] == '0a2e22a2-a7a1-47d4-9249-3eaafe1c815b']\n",
    "#row\n",
    "\n",
    "#7. Themes from dynamodb foundational\n",
    "\n",
    "# 7.1. Initialize DynamoDB client\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "table = 'foundational'\n",
    "\n",
    "# 7.2. Batch read from DynamoDB (since it is not too large)\n",
    "def scan_table(table_name):\n",
    "    paginator = dynamodb.get_paginator('scan')\n",
    "    items = []\n",
    "    for page in paginator.paginate(TableName=table_name):\n",
    "        items.extend(page['Items'])\n",
    "    return items\n",
    "\n",
    "all_items = scan_table(table)\n",
    "cleaned = [{'uuid': item['uuid']['S'], 'tag': item['loc']['S']} for item in all_items]\n",
    "df_theme = pd.DataFrame(cleaned)\n",
    "\n",
    "# 7.3. Merge with df_en using features_properties_id\n",
    "df_en = df_en.merge(df_theme, how='left', left_on='features_properties_id', right_on='uuid')\n",
    "\n",
    "# 7.4. Append the tag to the geo_theme list if not already present\n",
    "def append_theme(existing, tag):\n",
    "    try:\n",
    "        if not isinstance(existing, list):\n",
    "            existing = []\n",
    "        if isinstance(tag, str) and tag and tag not in existing:\n",
    "            existing.append(tag)\n",
    "        return existing\n",
    "    except Exception:\n",
    "        return existing\n",
    "\n",
    "df_en[\"features_properties_geo_theme\"] = df_en.apply(\n",
    "    lambda row: append_theme(row[\"features_properties_geo_theme\"], row[\"tag\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 7.5. Drop helper columns from step 2\n",
    "df_en.drop(columns=['uuid', 'tag'], inplace=True)\n",
    "\n",
    "# 8. Prints for debugging\n",
    "\n",
    "#df_fitered = df_en[df_en['features_properties_id'] == '0a2e22a2-a7a1-47d4-9249-3eaafe1c815b']\n",
    "#df_fitered\n",
    "\n",
    "#check if there are any records not mapped to a geo-ca theme\n",
    "#filtered = df_en[\n",
    "#    df_en[\"features_properties_geo_theme\"].isnull() | \n",
    "#    (df_en[\"features_properties_geo_theme\"].apply(lambda x: x == [] or x == ''))\n",
    "#]\n",
    "\n",
    "# Print all columns of those rows\n",
    "#filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a1f61-7c7e-4708-9b0c-fbb29c7af679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 use additional themes from the dynamodb table called 'theme'\n",
    "\n",
    "# 1.1. Initialize DynamoDB client\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "table = 'theme'\n",
    "\n",
    "# 1.2. Batch read from DynamoDB (since it is not too large)\n",
    "def scan_table(table_name):\n",
    "    paginator = dynamodb.get_paginator('scan')\n",
    "    items = []\n",
    "    for page in paginator.paginate(TableName=table_name):\n",
    "        items.extend(page['Items'])\n",
    "    return items\n",
    "\n",
    "all_items = scan_table(table)\n",
    "cleaned = [{'uuid': item['uuid']['S'], 'tag': item['tag']['S']} for item in all_items]\n",
    "df_theme = pd.DataFrame(cleaned)\n",
    "print(df_theme)\n",
    "\n",
    "# 1.3. Merge with df_en using features_properties_id\n",
    "df_en = df_en.merge(df_theme, how='left', left_on='features_properties_id', right_on='uuid')\n",
    "\n",
    "# 1.4. Append the tag to the geo_theme list if not already present\n",
    "def append_theme(existing, tag):\n",
    "    try:\n",
    "        if not isinstance(existing, list):\n",
    "            existing = []\n",
    "        if tag in ['emergency', 'legal'] and tag not in existing:\n",
    "            existing.append(tag)\n",
    "        return existing\n",
    "    except Exception:\n",
    "        return existing\n",
    "\n",
    "df_en[\"features_properties_geo_theme\"] = df_en.apply(\n",
    "    lambda row: append_theme(row[\"features_properties_geo_theme\"], row[\"tag\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 1.5. Drop helper columns from step 2\n",
    "df_en.drop(columns=['uuid', 'tag'], inplace=True)\n",
    "\n",
    "# 2.1 Foundational\n",
    "dynamodb = boto3.client('dynamodb')\n",
    "table = 'foundational'\n",
    "\n",
    "# 2.2. Batch read from DynamoDB (since it is not too large)\n",
    "def scan_table(table_name):\n",
    "    paginator = dynamodb.get_paginator('scan')\n",
    "    items = []\n",
    "    for page in paginator.paginate(TableName=table_name):\n",
    "        items.extend(page['Items'])\n",
    "    return items\n",
    "\n",
    "all_items = scan_table(table)\n",
    "cleaned = [{'uuid': item['uuid']['S'], 'tag': item['loc']['S']} for item in all_items]\n",
    "df_theme = pd.DataFrame(cleaned)\n",
    "\n",
    "# 2.3. Merge with df_en using features_properties_id\n",
    "df_en = df_en.merge(df_theme, how='left', left_on='features_properties_id', right_on='uuid')\n",
    "\n",
    "# 2.4. Append the tag to the geo_theme list if not already present\n",
    "def append_theme(existing, tag):\n",
    "    try:\n",
    "        if not isinstance(existing, list):\n",
    "            existing = []\n",
    "        if isinstance(tag, str) and tag and tag not in existing:\n",
    "            existing.append(tag)\n",
    "        return existing\n",
    "    except Exception:\n",
    "        return existing\n",
    "\n",
    "df_en[\"features_properties_geo_theme\"] = df_en.apply(\n",
    "    lambda row: append_theme(row[\"features_properties_geo_theme\"], row[\"tag\"]),\n",
    "    axis=1\n",
    ")\n",
    "df_en.drop(columns=['uuid', 'tag'], inplace=True)\n",
    "\n",
    "# 3. Create a new column called features_properties_foundational\n",
    "def is_foundational(theme):\n",
    "    try:\n",
    "        import json\n",
    "        \n",
    "        # If it's a JSON string, parse it\n",
    "        if isinstance(theme, str):\n",
    "            try:\n",
    "                theme_parsed = json.loads(theme)\n",
    "            except Exception:\n",
    "                # fallback: treat as plain string\n",
    "                theme_parsed = theme\n",
    "        else:\n",
    "            theme_parsed = theme\n",
    "\n",
    "        # Normalize to a list\n",
    "        if isinstance(theme_parsed, list):\n",
    "            themes = [str(t).strip().lower() for t in theme_parsed if t is not None]\n",
    "        elif isinstance(theme_parsed, str):\n",
    "            themes = [theme_parsed.strip().lower()]\n",
    "        else:\n",
    "            themes = []\n",
    "\n",
    "        return \"foundational\" in themes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing theme: {e}\")\n",
    "        return False\n",
    "\n",
    "df_en[\"features_properties_foundational\"] = df_en[\"features_properties_geo_theme\"].apply(is_foundational)\n",
    "\n",
    "# Normalize boolean to lowercase string \"true\"/\"false\" if you want consistency with your mappable column\n",
    "df_en[\"features_properties_foundational\"] = df_en[\"features_properties_foundational\"].astype(str).str.lower()\n",
    "\n",
    "# Test\n",
    "#row = df_en[df_en['features_properties_id'] == '0a2e22a2-a7a1-47d4-9249-3eaafe1c815b']\n",
    "#row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765ce5d-5932-4a16-b907-57d55ce6ab94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#7. Convert ISO themes column to a vector for OpenSearch indexing to support aggregates\n",
    "\n",
    "df_en['features_properties_topicCategory'] = df_en['features_properties_topicCategory'].apply(\n",
    "    lambda x: [s.strip() for s in x.split(',')] if isinstance(x, str) else x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "df_en[\"features_properties_keywords_en\"] = df_en[\"features_properties_keywords_en\"].apply(\n",
    "    lambda x: [s.strip() for s in x.split(',')] if isinstance(x, str) else x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "df_en[\"features_properties_keywords_fr\"] = df_en[\"features_properties_keywords_fr\"].apply(\n",
    "    lambda x: [s.strip() for s in x.split(',')] if isinstance(x, str) else x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "df_en\n",
    "\n",
    "df_fitered = df_en[df_en['features_properties_id'] == 'd256b422-2834-40a2-9f0c-bd5fc32781b2']\n",
    "#df_fitered[features_properties_contact]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bf684-96b8-4470-b7b2-fb7d00e0fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show full content in columns\n",
    "print(df_fitered['features_properties_contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8471fe1-c9d6-4053-a9f5-9e738e39ba16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector = df_en['vector']\n",
    "print(type(vector))\n",
    "# Check for null values\n",
    "has_null = vector.isnull().any()\n",
    "print(f\"Series has null values: {has_null}\")\n",
    "\n",
    "json_en = df_en.to_dict(\"records\")\n",
    "print(type(json_en))\n",
    "#print(json_en[0])\n",
    "\n",
    "# Extract the 'vector' values\n",
    "vectors = [item['vector'] for item in json_en]\n",
    "print(type(vectors))\n",
    "#print(vectors[0])\n",
    "\n",
    "#heck if there is null values \n",
    "import numpy as np \n",
    "array = np.array(vectors, dtype=object)\n",
    "has_null = np.any(array == None)\n",
    "print(f\"List of lists has null values: {has_null}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21091b5a-ea88-42f3-b544-fb703f1bd138",
   "metadata": {},
   "source": [
    "Under the cloudformation template 'geocore-semantic-search-with-opensearch-stage; Output tab, find the values for region, aos_host, and os_secret_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4ac36-ea4d-4cbe-86ac-55d210620fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e329f-2b5e-463c-a7d7-3506550b5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "from opensearchpy.helpers import bulk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_awsauth_from_secret(region, secret_id):\n",
    "    \"\"\"\n",
    "    Retrieves AWS opensearh credentials stored in AWS Secrets Manager.\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client('secretsmanager', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        response = client.get_secret_value(SecretId=secret_id)\n",
    "        secret = json.loads(response['SecretString'])\n",
    "        \n",
    "        master_username = secret['username']\n",
    "        master_password = secret['password']\n",
    "\n",
    "        return (master_username, master_password)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving secret: {e}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def create_opensearch_connection(aos_host, awsauth):\n",
    "    try:\n",
    "        # Create the OpenSearch client\n",
    "        aos_client = OpenSearch(\n",
    "            hosts=[{'host': aos_host, 'port': 443}],\n",
    "            http_auth=awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "            # timeout=60,  # Set a higher timeout value\n",
    "            # max_retries=10,  # Increase the number of retries\n",
    "            # retry_on_timeout=True\n",
    "        )\n",
    "        # Print the client to confirm the connection\n",
    "        print(\"Connection to OpenSearch established:\", aos_client)\n",
    "        return aos_client\n",
    "    except Exception as e:\n",
    "        print(\"Failed to connect to OpenSearch:\", e)\n",
    "        return None\n",
    "\n",
    "def delete_aos_index_if_exists(aos_client, index_to_delete):\n",
    "    \"\"\"\n",
    "    Deletes the specified index if it exists.\n",
    "\n",
    "    :param aos_client: An instance of OpenSearch client.\n",
    "    :param index_to_delete: The name of the index to delete.\n",
    "    \"\"\"\n",
    "    # List all indexes and check if the specified index exists\n",
    "    all_indices = aos_client.cat.indices(format='json')\n",
    "    existing_indices = [index['index'] for index in all_indices]\n",
    "    print(\"Current indexes:\", existing_indices)\n",
    "\n",
    "    if index_to_delete in existing_indices:\n",
    "        # Delete the specified index\n",
    "        try:\n",
    "            response = aos_client.indices.delete(index=index_to_delete)\n",
    "            print(f\"Deleted index: {index_to_delete}\")\n",
    "            print(\"Response:\", response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting index {index_to_delete}:\", e)\n",
    "    else:\n",
    "        print(f\"Index {index_to_delete} does not exist.\")\n",
    "\n",
    "    # List all indexes again to confirm deletion\n",
    "    all_indices_after_deletion = aos_client.cat.indices(format='json')\n",
    "    existing_indices_after_deletion = [index['index'] for index in all_indices_after_deletion]\n",
    "    print(\"Indexes after deletion attempt:\", existing_indices_after_deletion)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def safe_json(raw, default):\n",
    "    \"\"\"Safely JSON-decode a field.\"\"\"\n",
    "    try:\n",
    "        if raw is None:\n",
    "            return default\n",
    "        return json.loads(raw)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "\n",
    "def load_data_to_opensearch_index_v2(df_en, aos_client, index_name, chunk_size=100):\n",
    "\n",
    "    start = time.time()\n",
    "    json_en = df_en.to_dict(\"records\")\n",
    "\n",
    "    # Check vectors\n",
    "    vectors = [item.get(\"vector\") for item in json_en]\n",
    "    array = np.array(vectors, dtype=object)\n",
    "    has_null = np.any(array == None)\n",
    "    print(f\"vector has null values: {has_null}\")\n",
    "\n",
    "    print(f\"Preparing {len(json_en)} documents...\")\n",
    "\n",
    "\n",
    "    def generate_actions():\n",
    "        \"\"\"Generate bulk actions document-by-document.\"\"\"\n",
    "        for x in json_en:\n",
    "\n",
    "            # Geometry\n",
    "            bounding_box = safe_json(x.get('features_geometry_coordinates'), [])\n",
    "            coordinates = {\"type\": \"Polygon\", \"coordinates\": bounding_box}\n",
    "\n",
    "            # Build source doc (same fields as your original)\n",
    "            src = {\n",
    "                'id': x.get('features_properties_id', ''),\n",
    "                'coordinates': coordinates,\n",
    "                'title_en': x.get('features_properties_title_en', ''),\n",
    "                'title_fr': x.get('features_properties_title_fr', ''),\n",
    "                'description_en': x.get('features_properties_description_en', ''),\n",
    "                'description_fr': x.get('features_properties_description_fr', ''),\n",
    "                'published': x.get('features_properties_date_published_date', ''),\n",
    "                'keywords_en': x.get('features_properties_keywords_en', ''),\n",
    "                'keywords_fr': x.get('features_properties_keywords_fr', ''),\n",
    "                'options': safe_json(x.get('features_properties_options'), []),\n",
    "                'contact': safe_json(x.get('features_properties_contact'), []),\n",
    "                'cited': safe_json(x.get('features_properties_cited'), []),\n",
    "                'topicCategory': x.get('features_properties_topicCategory', ''),\n",
    "                'created': x.get('features_properties_date_created_date', ''),\n",
    "                'spatialRepresentation': x.get('features_properties_spatialRepresentation', ''),\n",
    "                'type': x.get('features_properties_type', ''),\n",
    "                'temporalExtent': x.get('temporalExtent', ''),\n",
    "                'graphicOverview': safe_json(x.get('features_properties_graphicOverview'), []),\n",
    "                'language': x.get('features_properties_language', ''),\n",
    "                'organisation': x.get('features_properties_org', ''),\n",
    "                'theme': x.get('features_properties_geo_theme', ''),\n",
    "                'mappable': x.get('features_properties_mappable', ''),\n",
    "                'foundational': x.get('features_properties_foundational', ''),\n",
    "                'popularity': int(x.get('features_popularity', 0)),\n",
    "                'systemName': x.get('features_properties_sourceSystemName', ''),\n",
    "                'eoCollection': x.get('features_properties_eoCollection', ''),\n",
    "                'eoFilters': safe_json(x.get('features_properties_eoFilters'), []),\n",
    "                \"vector\": x.get(\"vector\", \"\")\n",
    "            }\n",
    "\n",
    "            yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_id\": x.get('features_properties_id'),\n",
    "                \"_source\": src\n",
    "            }\n",
    "\n",
    "\n",
    "    print(f\"Starting bulk indexing with chunk_size={chunk_size}...\")\n",
    "\n",
    "    try:\n",
    "        success, errors = bulk(\n",
    "            aos_client,\n",
    "            generate_actions(),\n",
    "            chunk_size=chunk_size,\n",
    "            max_retries=3,\n",
    "            request_timeout=120,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"\\nâŒ Bulk indexing failed:\")\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    print(f\"âœ” Bulk indexing done. Successful: {success}\")\n",
    "    print(f\"â— Errors returned by bulk(): {errors}\")\n",
    "    print(f\"â± Total time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "def load_data_to_opensearch_index(df_en, aos_client, index_name, log_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Index data from a pandas DataFrame to an OpenSearch index.\n",
    "\n",
    "    Parameters:\n",
    "    - df_en: DataFrame containing the data to index.\n",
    "    - aos_client: OpenSearch client.\n",
    "    - index_name: Name of the OpenSearch index to which the data will be indexed.\n",
    "    - log_level: Logging level, defaults to \"INFO\". Set to \"DEBUG\" for detailed logs.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert DataFrame to a list of dictionaries (JSON)\n",
    "    json_en = df_en.to_dict(\"records\")\n",
    "    \n",
    "    # check if vector has null values \n",
    "    vectors = [item['vector'] for item in json_en]\n",
    "    import numpy as np \n",
    "    array = np.array(vectors, dtype=object)\n",
    "    has_null = np.any(array == None)\n",
    "    print(f\"vector has null values: {has_null}\")\n",
    "\n",
    "    # Index the data\n",
    "    for x in tqdm(json_en, desc=\"Indexing Records\"):\n",
    "        try:\n",
    "            bounding_box = json.loads(x.get('features_geometry_coordinates', '[]'))\n",
    "            coordinates = {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": bounding_box\n",
    "            }\n",
    "\n",
    "            document = {\n",
    "                'id': x.get('features_properties_id', ''),\n",
    "                'coordinates': coordinates,\n",
    "                'title_en': x.get('features_properties_title_en', ''),\n",
    "                'title_fr': x.get('features_properties_title_fr', ''),\n",
    "                'description_en': x.get('features_properties_description_en', ''),\n",
    "                'description_fr': x.get('features_properties_description_fr', ''),\n",
    "                'published': x.get('features_properties_date_published_date', ''),\n",
    "                'keywords_en': x.get('features_properties_keywords_en', ''),\n",
    "                'keywords_fr': x.get('features_properties_keywords_fr', ''),\n",
    "                'options': json.loads(x.get('features_properties_options', '[]')),\n",
    "                'contact': json.loads(x.get('features_properties_contact', '[]')),\n",
    "                'cited': json.loads(x.get('features_properties_cited', '[]')),\n",
    "                'topicCategory': x.get('features_properties_topicCategory', ''),\n",
    "                'created': x.get('features_properties_date_created_date', ''),\n",
    "                'spatialRepresentation': x.get('features_properties_spatialRepresentation', ''),\n",
    "                'type': x.get('features_properties_type', ''),\n",
    "                'temporalExtent': x.get('temporalExtent', ''),\n",
    "                'graphicOverview': json.loads(x.get('features_properties_graphicOverview', '[]')),\n",
    "                'language': x.get('features_properties_language', ''),\n",
    "                'organisation': x.get('features_properties_org', ''),\n",
    "                'theme': x.get('features_properties_geo_theme',''),\n",
    "                'mappable': x.get('features_properties_mappable',''),\n",
    "                'foundational': x.get('features_properties_foundational',''),\n",
    "                'type': x.get('features_properties_type',''),\n",
    "                'popularity': int(x.get('features_popularity', '0')),\n",
    "                'systemName': x.get('features_properties_sourceSystemName', ''),\n",
    "                'eoCollection': x.get('features_properties_eoCollection', ''),\n",
    "                'eoFilters': json.loads(x.get('features_properties_eoFilters', '[]')),\n",
    "                \"vector\":x.get(\"vector\", \"\")\n",
    "            }\n",
    "\n",
    "            if log_level == \"DEBUG\":\n",
    "                print((json.dumps(document, indent=4)))\n",
    "\n",
    "            aos_client.index(index=index_name, body=document)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    # Final record count check\n",
    "    try:\n",
    "        res = client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "        print(f\"Total documents in index: {res['hits']['total']['value']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving document count: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644d506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "REGION = \"ca-central-1\"\n",
    "AOS_HOST = \"REDACTED.ca-central-1.es.amazonaws.com\"\n",
    "os_secret_id = \"OpenSearchSecret-geocore-semantic-search-with-opensearch-prod\"\n",
    "\n",
    "\n",
    "def connect_to_opensearch(REGION, AOS_HOST):\n",
    "    try:        \n",
    "        credentials = boto3.Session().get_credentials()\n",
    "        if not credentials:\n",
    "            raise NoCredentialsError()\n",
    "        awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, REGION, 'es', session_token=credentials.token)\n",
    "\n",
    "        os_client = OpenSearch(\n",
    "            hosts=[{'host': AOS_HOST, 'port': 443}],\n",
    "            http_auth=awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )\n",
    "        return os_client\n",
    "    except NoCredentialsError:\n",
    "        print(\"Missing AWS credentials.\")\n",
    "        return None\n",
    "    except ConnectionError as e:\n",
    "        print(f\"Failed to connect to OpenSearch: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "aos_client = connect_to_opensearch(REGION, AOS_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba38706",
   "metadata": {},
   "source": [
    "### 7. Create a index in Amazon Opensearch Service \n",
    "The following index setting is configuring an OpenSearch index to support k-nearest neighbor (KNN) searches with specific characteristics. KNN is a feature that allows for similarity searches, finding the \"nearest\" documents in a high-dimensional space. This setting is crucial for enabling vector-based searches, where vectors represent document features in a multidimensional space.\n",
    "\n",
    "How k-NN search works:K-NN search works by calculating the distance between a query vector and the vectors in the dataset to find the closest matches. OpenSearch stores these vectors in an index and uses specialized algorithms (like HNSW, Hierarchical Navigable Small World graphs) to perform efficient similarity search at scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f67fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = \"minilm-knn-multilingual\"\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True, #This enables the k-nearest neighbor (KNN) search capability on the index.\n",
    "        \"index.knn.space_type\": \"cosinesimil\", #cosine similarity \n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 384,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"coordinates\":{\n",
    "              \"type\": \"geo_shape\", \n",
    "              \"store\": True \n",
    "            }  \n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced5690-ccbb-4b55-a4fc-5baef8fa98a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Delete index if it exists \n",
    "delete_aos_index_if_exists(aos_client, index_to_delete=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1e8a0-8fee-43b3-a63c-efa709395459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a index \n",
    "aos_client.indices.create(index=index_name,body=knn_index,ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdbefe-1dd5-4a63-a96d-33ec2bd5cccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load data to OpenSearch Index \n",
    "load_data_to_opensearch_index(df_en, aos_client, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec581fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(f\"Records loaded into the index {index_name} is {res['hits']['total']['value']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01160d3f",
   "metadata": {},
   "source": [
    "### 8. Test \"Semantic Search\" \n",
    "\n",
    "Now that we have vector in OpenSearch and a vector for our query question, let's perform a KNN search in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf1654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name=\"minilm-knn-multilingual\"\n",
    "INPUT = \"protocole de mesure de la pollution de lâ€™air\"\n",
    "search_vector = sentence_to_vector(INPUT)\n",
    "print(index_name)\n",
    "\n",
    "query={\n",
    "    \"size\": 20,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"vector\":{\n",
    "                \"vector\":search_vector,\n",
    "                \"k\":20\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=index_name, size=20,body=query,request_timeout=55)\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['_source']['title_en'],hit['_source']['id']]\n",
    "    query_result.append(row)\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"relevancy_score\",\"title\",'uuid'])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a848bf-8a77-4141-9845-0cc70dd8bcc3",
   "metadata": {},
   "source": [
    "### 9. Test \"Semantic Search\" using the model endpoints\n",
    "\n",
    "Now that we have vector in OpenSearch and a vector for our query question, let's perform a KNN search in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6c041-2009-4311-86d8-e0e5aba9d189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a boto3 client for SageMaker\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# Initialize a boto3 client for SageMaker\n",
    "sagemaker_client = boto3.client('sagemaker', region_name='ca-central-1')  # Specify the AWS region\n",
    "\n",
    "def list_sagemaker_endpoints():\n",
    "    \"\"\"List all SageMaker endpoints\"\"\"\n",
    "    try:\n",
    "        # Get the list of all SageMaker endpoints\n",
    "        response = sagemaker_client.list_endpoints(SortBy='Name')\n",
    "        print(\"Listing SageMaker Endpoints:\")\n",
    "        for endpoint in response['Endpoints']:\n",
    "            print(f\"Endpoint Name: {endpoint['EndpointName']}, Status: {endpoint['EndpointStatus']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing SageMaker endpoints: {e}\")\n",
    "\n",
    "def invoke_sagemaker_endpoint_ft(endpoint_name, payload):\n",
    "    \"\"\"Invoke a SageMaker endpoint to get predictions with ContentType='application/json'.\"\"\"\n",
    "    # Initialize the runtime SageMaker client\n",
    "    runtime_client = boto3.client('runtime.sagemaker', region_name='ca-central-1')  \n",
    "    try:\n",
    "        \"\"\"\n",
    "        if not isinstance(payload, str):\n",
    "            payload = str(payload)\n",
    "        \"\"\"\n",
    "        # Invoke the SageMaker endpoint\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        # Decode the response\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        return (result)\n",
    "        #print(f\"Prediction from {endpoint_name}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "\n",
    "def invoke_sagemaker_endpoint_pretrain(endpoint_name, payload):\n",
    "    \"\"\"Invoke a SageMaker endpoint to get predictions with ContentType='text/plain'.\"\"\"\n",
    "    # Initialize the runtime SageMaker client\n",
    "    runtime_client = boto3.client('runtime.sagemaker', region_name='ca-central-1')  \n",
    "\n",
    "    try:\n",
    "        # Ensure payload is a string, since ContentType is 'text/plain'\n",
    "        if not isinstance(payload, str):\n",
    "            payload = str(payload)\n",
    "        \n",
    "        # Invoke the SageMaker endpoint\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='text/plain',\n",
    "            Body=payload\n",
    "        )\n",
    "        \n",
    "        # Decode the response\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        return (result)\n",
    "        #print(f\"Prediction from {endpoint_name}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6459d-cde4-42cd-b8f7-aabf5f1dff98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_sagemaker_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502cd7a-0e64-43c5-bab6-910565e7a94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = 'semantic-search-pretrain-all-MiniLM-L6-v2-1743177448'\n",
    "payload = \"inondation\"\n",
    "vector = invoke_sagemaker_endpoint_pretrain(endpoint_name, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1e1b8-fc23-48e0-a67c-48cf635a7419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name=\"minilm-knn-2\"\n",
    "\n",
    "query={\n",
    "    \"size\": 10,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"vector\":{\n",
    "                \"vector\":vector,\n",
    "                \"k\":10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=index_name, size=20,body=query,request_timeout=55)\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[hit['_id'],hit['_score'],hit['_source']['title'],hit['_source']['id']]\n",
    "    query_result.append(row)\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\"_id\",\"relevancy_score\",\"title\",'uuid'])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72615ad-fd2e-489f-9367-afd637f62474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
